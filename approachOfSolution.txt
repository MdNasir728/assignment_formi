
- What was your initial thought process when you first read the problem statement, and how did you break it down into smaller, manageable parts?

When I first read the problem, I realized the key task was to find the nearest hotel properties to a user-input location — even if the input contained typos — and return results within 2 seconds. I broke the problem into five main components:

Query Understanding and Error Correction
Users might type city names incorrectly, so I needed a way to recognize and correct these typos quickly.

Location Geocoding
Once the location is corrected, I needed to find its corresponding latitude and longitude efficiently.

Distance Calculation
With the input location's coordinates and the coordinates of all hotel properties, I needed to calculate distances using the Haversine formula.

Filtering Results
After calculating distances, I needed to filter and return only the properties within 50 km.

Building a Fast API
The system should expose an endpoint where a user can enter a location (possibly with typos) and receive a list of nearby properties in under 2 seconds.

- What specific tools, libraries, or online resources did you use to develop your solution, and why did you choose them over other options

FastAPI: Chosen for its high performance and ease of building async APIs. Perfect for real-time responses.

uvicorn: Lightweight ASGI server compatible with FastAPI for fast deployment and testing.

pandas: Used for handling and processing the in-memory CSV of city coordinates and hotel data.

geopy: Provided geodesic distance calculation via the Haversine formula with good accuracy.

fuzzywuzzy: Used for fuzzy matching.

city-coordinates CSV: A preloaded file containing Indian cities with latitude and longitude enabled fast, offline lookups without needing external API calls.


- Describe a key challenge you faced while solving this problem and how you arrived at the final solution?

The biggest challenge was handling misspelled location inputs while maintaining a fast response time (under 2 seconds).
Fuzzy matching against a list of city names — even just 150 — can still introduce delays, especially when accuracy is a priority.

To solve this:

I loaded a cleaned list of 150 Indian city names along with their coordinates from a CSV file into memory at startup to allow for quick access.

I used fuzzywuzzy for typo correction, it provided decent accuracy for this smaller dataset.

I implemented a simple caching mechanism (created a dictionary of all the cities at 1st time and used it) for common or repeated queries to avoid redundant fuzzy matching and coordinate lookups.


This setup helped maintain low latency and ensured the system returned accurate, typo-tolerant results within the required response time.

- If you had more time, what improvements or alternative approaches would you explore, and why do you think they might be valuable?

Use Embeddings for Semantic Matching
I’d explore converting user queries into vector embeddings using models from OpenAI or HuggingFace. Matching these embeddings with precomputed city embeddings would allow the system to handle vague or indirect queries like “hill station near Delhi” more intelligently, going beyond simple typo correction.

Pre-calculate distances using GeoHash or KD-Tree
By implementing spatial indexing techniques like GeoHash or KD-Tree, I could narrow down candidate cities or hotel properties before performing distance calculations. This would reduce the computational load and improve overall performance, especially as the number of cities or properties increases.

Use Redis + Vector Search
Redis could be used to store city embeddings and support lightning-fast semantic and typo-tolerant nearest-neighbor lookups. This could complement or even replace fuzzy matching, making the system more scalable and real-time.

Leverage LLMs (optional)
For ambiguous, conversational, or exploratory queries (e.g., “a peaceful place near Manali”), an LLM could be used to extract intent and suggest relevant locations before performing geolocation and filtering. This would enhance the user experience by allowing more natural interaction.

Use Google Maps API for Geocoding
While I used a local CSV for city coordinates in the current version, given more time and budget, I’d consider integrating the Google Maps Geocoding API. It offers high accuracy, especially for small or lesser-known towns, and would eliminate the need for maintaining a manual city-coordinates file.

These additions would significantly improve the system’s accuracy, flexibility, and scalability, especially as it expands to support more cities, user input styles, or complex queries.


